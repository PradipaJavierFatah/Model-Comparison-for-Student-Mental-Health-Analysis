{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PradipaJavierFatah/Model_Comparison_for_Student_Mental_Health_Analysis/blob/main/Model_Comparison_for_Student_Mental_Health_Analysis_KNN_vs_Logistic_Regression_vs_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Comparison for Student Mental Health Analysis KNN vs Logistic Regression vs Naive Bayes**"
      ],
      "metadata": {
        "id": "-7DntPnQiCPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. KNN**"
      ],
      "metadata": {
        "id": "ARu95swYjg_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=0)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the SMOTE-transformed data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_smote, y_smote, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and KNN classifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define a hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__n_neighbors': [3, 5, 7],  # Number of neighbors\n",
        "    'classifier__weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
        "}\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy for KNN with SMOTE: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuCGhtFHY2cB",
        "outputId": "0f5f7754-ef45-4ce5-a444-6cd977e79cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 6225 samples\n",
            "Validation set size: 778 samples\n",
            "Testing set size: 779 samples\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best Hyperparameters: {'classifier__n_neighbors': 7, 'classifier__weights': 'distance'}\n",
            "Validation set accuracy: 0.56\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.60      0.50      0.54       404\n",
            "        True       0.54      0.64      0.59       374\n",
            "\n",
            "    accuracy                           0.56       778\n",
            "   macro avg       0.57      0.57      0.56       778\n",
            "weighted avg       0.57      0.56      0.56       778\n",
            "\n",
            "Testing set accuracy for KNN with SMOTE: 0.59\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.59      0.55      0.57       390\n",
            "        True       0.58      0.62      0.60       389\n",
            "\n",
            "    accuracy                           0.59       779\n",
            "   macro avg       0.59      0.59      0.58       779\n",
            "weighted avg       0.59      0.59      0.58       779\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. NAIVE BAYES"
      ],
      "metadata": {
        "id": "MMlTiv3CFOiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=0)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_smote, y_smote, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and GaussianNB\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation (no hyperparameters to tune for GaussianNB)\n",
        "grid_search = GridSearchCV(pipeline, {}, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found (will be empty for GaussianNB)\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy for naive bayes: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy for naive bayes: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUhTvs8vHUuW",
        "outputId": "81831166-5d5f-4e02-cb7c-e5ae66a3d630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 6225 samples\n",
            "Validation set size: 778 samples\n",
            "Testing set size: 779 samples\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Best Hyperparameters: {}\n",
            "Validation set accuracy for naive bayes: 0.57\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.61      0.49      0.54       404\n",
            "        True       0.54      0.66      0.59       374\n",
            "\n",
            "    accuracy                           0.57       778\n",
            "   macro avg       0.57      0.57      0.57       778\n",
            "weighted avg       0.58      0.57      0.57       778\n",
            "\n",
            "Testing set accuracy for naive bayes: 0.58\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.59      0.48      0.53       390\n",
            "        True       0.56      0.67      0.61       389\n",
            "\n",
            "    accuracy                           0.58       779\n",
            "   macro avg       0.58      0.58      0.57       779\n",
            "weighted avg       0.58      0.58      0.57       779\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "N5gMk2KZFH5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=0)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_smote, y_smote, test_size=0.5, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and LogisticRegression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(random_state=0))\n",
        "])\n",
        "\n",
        "# Define a simplified hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']  # Optimization algorithm\n",
        "}\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation (reduced from 5)\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "QGGmyP-Kb1qK",
        "outputId": "15c9a87f-c82b-431b-87b2-0a7864b883e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 3891 samples\n",
            "Validation set size: 3112 samples\n",
            "Testing set size: 779 samples\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__solver': 'liblinear'}\n",
            "Validation set accuracy: 0.55\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.56      0.53      0.55      1571\n",
            "        True       0.55      0.58      0.56      1541\n",
            "\n",
            "    accuracy                           0.55      3112\n",
            "   macro avg       0.55      0.55      0.55      3112\n",
            "weighted avg       0.55      0.55      0.55      3112\n",
            "\n",
            "Testing set accuracy: 0.59\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.61      0.57      0.59       407\n",
            "        True       0.56      0.60      0.58       372\n",
            "\n",
            "    accuracy                           0.59       779\n",
            "   macro avg       0.59      0.59      0.59       779\n",
            "weighted avg       0.59      0.59      0.59       779\n",
            "\n"
          ]
        }
      ]
    }
  ]
}